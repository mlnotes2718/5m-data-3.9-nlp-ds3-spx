{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "229a26c1-c306-4ed0-864f-e29bb064efdd",
   "metadata": {},
   "source": [
    "# Introduction to Natural Language Processing (NLP)\n",
    "\n",
    "Natural Language Processing, or NLP, is a field at the intersection of computer science, artificial intelligence, and linguistics. It involves the development of algorithms and systems that enable computers to understand, interpret, and generate human language. NLP is a way for computers to analyze, understand, and derive meaning from human language in a smart and useful way.\n",
    "\n",
    "NLP helps resolve ambiguity in language and adds useful numeric structure to the data for many downstream applications, such as speech recognition or text analytics.\n",
    "\n",
    "![nlp](../assets/nlp.png)\n",
    "\n",
    "## Challenges in NLP\n",
    "\n",
    "### Part-of-Speech Tagging\n",
    "\n",
    "![pos](../assets/pos.jpeg)\n",
    "\n",
    "Part-of-speech (POS) tagging is the process of assigning a part of speech to each word in a text, such as noun, verb, adjective, etc. This is challenging because:\n",
    "\n",
    "- **Ambiguity**: A word can have multiple parts of speech based on the context. For example, \"book\" can be a noun (\"I read a book\") or a verb (\"Book a table\").\n",
    "- **Contextual Use**: Words may be used in a figurative sense, which can confuse POS taggers.\n",
    "- **New Words**: New words, slang, and jargon keep emerging, and POS taggers need regular updates to handle them.\n",
    "\n",
    "### Text Segmentation\n",
    "\n",
    "Text segmentation involves dividing text into meaningful units, such as sentences or topics. Challenges include:\n",
    "\n",
    "![senseg](../assets/sentence_segmentation.jpeg)\n",
    "\n",
    "- **Sentence Boundary Detection**: Punctuation marks like periods can be used for abbreviations, decimals, etc., and not always to end sentences.\n",
    "- **Tokenization**: Different languages and scripts have different tokenization rules, and some don't use whitespace.\n",
    "- **Topic Segmentation**: Identifying topic shifts in a text requires understanding of the content, which is a non-trivial task.\n",
    "\n",
    "### Word Sense Disambiguation\n",
    "\n",
    "Word sense disambiguation is the task of determining which sense of a word is active in a given context. Challenges include:\n",
    "\n",
    "- **Polysemy**: Many words have multiple meanings, and identifying the correct one is difficult without deep understanding.\n",
    "- **Limited Context**: Sometimes the surrounding text is not enough to determine the word sense.\n",
    "- **Lack of Resources**: For less-resourced languages, there might not be enough data to train disambiguation systems.\n",
    "\n",
    "> - Many plants and animals live in the rainforest.\n",
    "> - The manufacturing plant produced widgets.\n",
    "\n",
    "### Syntax Disambiguation\n",
    "\n",
    "Syntax disambiguation deals with the different ways in which words can be combined to form sentences. Challenges here include:\n",
    "\n",
    "- **Structural Ambiguity**: Sentences can often be parsed in multiple ways (\"I saw the man with the telescope\").\n",
    "- **Complex Constructions**: Some languages have free word order or allow for nested clauses, making parsing difficult.\n",
    "- **Idiomatic Expressions**: Phrases that don't follow standard syntax rules can confuse parsers.\n",
    "\n",
    "> - Annie hit a man with an umbrella.\n",
    "> - I shot an elephant in my pyjamas.\n",
    "> - The tourist saw the woman with a telescope.\n",
    "\n",
    "### Imperfect or Irregular Input\n",
    "\n",
    "Language is often messy and unpredictable, leading to challenges such as:\n",
    "\n",
    "- **Typos and Spelling Errors**: Mistakes in writing can lead to misinterpretation by NLP systems.\n",
    "- **Non-standard Language**: Use of slang, abbreviations, and non-standard grammar can be problematic.\n",
    "- **Multilingual Text**: Text containing multiple languages can complicate processing.\n",
    "\n",
    "## Applications of NLP\n",
    "\n",
    "- **Text Classification**: Assigning categories or labels to text, such as spam detection in email services.\n",
    "- **Machine Translation**: Translating text from one language to another, like Google Translate.\n",
    "- **Sentiment Analysis**: Identifying the sentiment of text, used in social media monitoring and market research.\n",
    "- **Chatbots and Virtual Assistants**: Powering conversational agents like Siri, Alexa, and customer service bots.\n",
    "- **Information Extraction**: Extracting structured information from unstructured text, such as named entity recognition.\n",
    "- **Summarization**: Generating a shortened version of a text, retaining its most important information.\n",
    "- **Speech Recognition**: Translating spoken language into text, used in voice user interfaces.\n",
    "- **Question Answering**: Building systems that automatically answer questions posed by humans in a natural language (ChatGPT).\n",
    "\n",
    "## Brief History of NLP\n",
    "\n",
    "The history of NLP generally starts in the 1950s, although work can be found from earlier periods. In 1950, Alan Turing published an article titled \"Computing Machinery and Intelligence\" which proposed what is now called the Turing test as a criterion of intelligence, a fundamental goal of natural language processing.\n",
    "\n",
    "### Milestones in the History of NLP\n",
    "\n",
    "- **1950s**: The era of symbolic NLP, rule-based systems that tried to encode human knowledge and grammar rules into computers.\n",
    "- **1960s**: Development of the first chatbot, ELIZA, and further work on machine translation.\n",
    "- **1970s-1980s**: The rise of computational linguistics and the development of more sophisticated models for handling syntax and semantics.\n",
    "- **1990s**: Introduction of statistical NLP, leveraging large amounts of data and statistical methods to process language.\n",
    "- **2000s**: The emergence of machine learning in NLP, with systems beginning to learn from data rather than relying on hand-coded rules.\n",
    "- **2010s-Present**: The rise of deep learning has revolutionized NLP, leading to the development of models like BERT and GPT that can handle complex language tasks with unprecedented accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85f49d1",
   "metadata": {},
   "source": [
    "# LLM on Why Need to Learn Old NLP Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4c784",
   "metadata": {},
   "source": [
    "<details>\n",
    "\n",
    "<summary>\n",
    "\n",
    "**Yes, exactly!** You've nailed the fundamental trade-off.\n",
    "\n",
    "Transformers (BERT, GPT, etc.) can theoretically perform **all** the tasks that older NLP techniques do, and generally with **much better accuracy**, but at the cost of:\n",
    "\n",
    "- **Computing resources** (GPU vs CPU)\n",
    "- **Speed** (milliseconds vs seconds)\n",
    "- **Memory** (MBs vs GBs)\n",
    "- **Cost** (pennies vs dollars)\n",
    "- **Energy** (watts vs kilowatts)\n",
    "\n",
    "</summary>\n",
    "\n",
    "## The Trade-off Table\n",
    "\n",
    "| Task | Old Method | Transformer | Accuracy Gain | Speed Loss |\n",
    "|------|------------|-------------|---------------|------------|\n",
    "| Language detection | FastText: 1ms | BERT: 50ms | Minimal (both ~99%) | 50x slower |\n",
    "| Sentiment analysis | Word2Vec+Classifier: 5ms | BERT: 100ms | +15% accuracy | 20x slower |\n",
    "| Text classification | FastText: 2ms | BERT: 80ms | +10-15% | 40x slower |\n",
    "| Named Entity Recognition | CRF: 10ms | BERT: 150ms | +20% accuracy | 15x slower |\n",
    "| Search (1M docs) | TF-IDF: 20ms | BERT: 30 minutes | Much better relevance | 90,000x slower |\n",
    "\n",
    "## Why the Big Speed Difference?\n",
    "\n",
    "**FastText:**\n",
    "```\n",
    "Input: \"I love dogs\"\n",
    "→ Lookup 3 embeddings in table: 0.1ms\n",
    "→ Average them: 0.01ms\n",
    "→ Classifier: 0.1ms\n",
    "Total: ~0.2ms (on CPU)\n",
    "```\n",
    "\n",
    "**BERT:**\n",
    "```\n",
    "Input: \"I love dogs\"\n",
    "→ Tokenization: 1ms\n",
    "→ Embedding lookup: 1ms\n",
    "→ 12 transformer layers with attention: 95ms (on GPU!)\n",
    "→ Classifier: 3ms\n",
    "Total: ~100ms (needs GPU)\n",
    "```\n",
    "\n",
    "## The Precision Difference\n",
    "\n",
    "**Example: Sentiment Analysis**\n",
    "\n",
    "**FastText approach:**\n",
    "```\n",
    "\"This movie is not bad\"\n",
    "→ Average: vec(\"this\") + vec(\"movie\") + vec(\"is\") + vec(\"not\") + vec(\"bad\")\n",
    "→ Heavy influence from vec(\"bad\") (negative)\n",
    "→ Prediction: Negative ❌ (WRONG!)\n",
    "Confidence: 65%\n",
    "```\n",
    "\n",
    "**BERT approach:**\n",
    "```\n",
    "\"This movie is not bad\"\n",
    "→ Attention mechanism understands \"not\" modifies \"bad\"\n",
    "→ Contextualized representation captures negation\n",
    "→ Prediction: Positive ✓ (CORRECT!)\n",
    "Confidence: 92%\n",
    "```\n",
    "\n",
    "**Results on benchmark:**\n",
    "- FastText: 82% accuracy\n",
    "- BERT: 94% accuracy\n",
    "- **+12% improvement** (but 50x slower)\n",
    "\n",
    "## Resource Requirements\n",
    "\n",
    "**Running 1 Million Classifications:**\n",
    "\n",
    "**FastText:**\n",
    "- Hardware: Basic CPU server ($50/month)\n",
    "- Time: ~2 minutes\n",
    "- Energy: ~0.1 kWh\n",
    "- Cost: ~$0.01\n",
    "\n",
    "**BERT:**\n",
    "- Hardware: GPU server ($500/month) or cloud GPU\n",
    "- Time: ~28 hours (or parallelize on many GPUs)\n",
    "- Energy: ~50 kWh\n",
    "- Cost: ~$20-100\n",
    "\n",
    "**That's a 2,000-10,000x cost difference!**\n",
    "\n",
    "## When the Precision Gain Matters\n",
    "\n",
    "### **Critical Applications (Use Transformer)**\n",
    "\n",
    "**Medical diagnosis from text:**\n",
    "- \"Patient does not show signs of infection\" vs \"Patient shows signs of infection\"\n",
    "- FastText might miss \"not\" → dangerous!\n",
    "- BERT's precision worth the cost\n",
    "\n",
    "**Legal document analysis:**\n",
    "- \"The contract is not binding\" vs \"The contract is binding\"\n",
    "- 12% error rate vs 6% error rate = significant legal risk\n",
    "\n",
    "**Financial fraud detection:**\n",
    "- Missing fraud costs millions\n",
    "- Extra precision worth extra compute\n",
    "\n",
    "### **Non-Critical Applications (Use FastText)**\n",
    "\n",
    "**Spam filtering:**\n",
    "- 82% accuracy catches most spam\n",
    "- Users can report misses\n",
    "- Speed matters more (process millions instantly)\n",
    "\n",
    "**Content categorization:**\n",
    "- \"Is this Sports or Politics?\"\n",
    "- FastText good enough for broad categories\n",
    "- Small errors acceptable\n",
    "\n",
    "**Language detection:**\n",
    "- FastText already 99%+ accurate\n",
    "- BERT barely improves but 50x slower\n",
    "\n",
    "## Real Production Decision\n",
    "\n",
    "**Company scenario:** Process 10M customer emails/day\n",
    "\n",
    "**Option 1: All BERT**\n",
    "```\n",
    "Cost: $500/day in GPU compute\n",
    "Accuracy: 94%\n",
    "Latency: 100ms per email\n",
    "Infrastructure: Complex GPU cluster\n",
    "```\n",
    "\n",
    "**Option 2: All FastText**\n",
    "```\n",
    "Cost: $10/day in CPU compute\n",
    "Accuracy: 82%\n",
    "Latency: 2ms per email\n",
    "Infrastructure: Simple CPU servers\n",
    "```\n",
    "\n",
    "**Option 3: Hybrid (Smart!)**\n",
    "```\n",
    "Stage 1 - FastText (filter obvious cases): 80% of emails\n",
    "→ Clear spam: instant classification\n",
    "→ Cost: $8/day\n",
    "\n",
    "Stage 2 - BERT (complex cases): 20% of emails\n",
    "→ Ambiguous emails get deep analysis\n",
    "→ Cost: $100/day\n",
    "\n",
    "Total: $108/day, 91% accuracy, 20ms average latency\n",
    "```\n",
    "\n",
    "**Best of both worlds!**\n",
    "\n",
    "## The \"Overkill\" Principle\n",
    "\n",
    "Using BERT for everything is like:\n",
    "\n",
    "- **Taking an airplane to go 2 blocks** (language detection)\n",
    "- **Using a supercomputer to add 2+2** (simple keyword matching)\n",
    "- **MRI scan for a paper cut** (obvious spam detection)\n",
    "\n",
    "The tool is more powerful, but **wasteful** for simple tasks.\n",
    "\n",
    "## Technical Capabilities Summary\n",
    "\n",
    "**Can transformers do everything older NLP does?**\n",
    "\n",
    "- ✅ Text classification - YES (better)\n",
    "- ✅ Sentiment analysis - YES (much better)\n",
    "- ✅ NER - YES (much better)\n",
    "- ✅ Language detection - YES (marginally better)\n",
    "- ✅ Search - YES (better relevance, but impractically slow at scale)\n",
    "- ✅ Clustering - YES (better semantic clusters)\n",
    "- ✅ Keyword extraction - YES (more context-aware)\n",
    "- ✅ Similarity - YES (better semantic understanding)\n",
    "\n",
    "**But should you use transformers for everything?**\n",
    "\n",
    "❌ Usually NO - because of cost, speed, complexity\n",
    "\n",
    "## The Future Trend\n",
    "\n",
    "**What's happening now:**\n",
    "\n",
    "1. **Model optimization:** Distilled BERT, quantization, pruning\n",
    "   - Make transformers smaller and faster\n",
    "   - \"Can we get BERT quality at FastText speed?\"\n",
    "\n",
    "2. **Specialized hardware:** TPUs, custom AI chips\n",
    "   - Make transformers cheaper to run\n",
    "\n",
    "3. **Hybrid architectures:** Best of both worlds\n",
    "   - Fast filtering + deep understanding\n",
    "\n",
    "4. **Edge deployment:** TinyBERT, MobileBERT\n",
    "   - Run simplified transformers on phones\n",
    "\n",
    "**Eventually:** As hardware improves and models get optimized, transformers might become fast/cheap enough to replace older techniques entirely. But we're not there yet!\n",
    "\n",
    "</details>\n",
    "\n",
    "## Your Understanding is Correct\n",
    "\n",
    "**Summary:**\n",
    "- ✅ Transformers CAN do everything with higher precision\n",
    "- ✅ BUT cost significantly more compute/speed/money\n",
    "- ✅ Production systems balance precision vs resources\n",
    "- ✅ Older techniques still valuable in the right context\n",
    "\n",
    "You've understood the fundamental trade-off perfectly! It's engineering pragmatism: use the right tool for the job, not always the most powerful tool.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e48448e",
   "metadata": {},
   "source": [
    "- https://claude.ai/share/36ceef3e-030b-40f6-8abc-8eac0ff1e64b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1313cd2-e658-4864-8a60-2f0dc263b96a",
   "metadata": {},
   "source": [
    "# Text Preprocessing\n",
    "\n",
    "Text preprocessing is a critical step in NLP. It involves preparing and cleaning text data for further analysis and modeling. The goal is to simplify the text and remove any noise that might distract the machine learning algorithms from understanding the core content.\n",
    "\n",
    "Raw text data is often messy and unstructured, with various issues:\n",
    "\n",
    "- Irrelevant characters and symbols\n",
    "- Inconsistent formatting\n",
    "- Typos and spelling errors\n",
    "- Diverse languages and slang\n",
    "- Stopwords (commonly used words that may not be useful in analysis)\n",
    "\n",
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down text into smaller units, called *tokens*. Tokens can be words, numbers, or punctuation marks. It's the first step in turning unstructured text into a form that can be analyzed.\n",
    "\n",
    "### White-space Tokenization\n",
    "\n",
    "This is the simplest form of tokenization. It splits the text by white spaces, including spaces, tabs, and new line characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01b71bf2-b565-4882-b092-5cca88a7b08a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'fun.']\n"
     ]
    }
   ],
   "source": [
    "def whitespace_tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "# Example usage:\n",
    "text = \"Natural language processing is fun.\"\n",
    "tokens = whitespace_tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2701c3-c486-4eca-af0a-3de088d7f194",
   "metadata": {},
   "source": [
    "### Punctuation-based Tokenization\n",
    "\n",
    "This method not only splits by white spaces but also considers punctuation marks as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96d89ec8-9f7c-4c8e-a0a1-933e194f5908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'fun']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def punctuation_tokenizer(text):\n",
    "    return re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "# Example usage:\n",
    "text = \"Natural language processing is fun!\"\n",
    "tokens = punctuation_tokenizer(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1dded96-252a-416d-8ea5-eee987770258",
   "metadata": {},
   "source": [
    "### Using NLP Libraries for Tokenization\n",
    "\n",
    "Libraries like `NLTK` and `spaCy` provide robust tokenization functions that handle edge cases and are more sophisticated than the simple white-space or punctuation-based methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "899927f5-01e9-4a61-bec1-e9664930e0eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/ds0/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2e5fa4",
   "metadata": {},
   "source": [
    "- https://www.nltk.org/api/nltk.tokenize.punkt.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5243c0d8-b212-4ab0-ab7f-5df1b1881403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural language processing is fun!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8dbed7-a987-49b2-97b3-3aec6b6f6442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Using cached https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Download the spaCy model\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22281b9",
   "metadata": {},
   "source": [
    "- https://spacy.io/models/en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "438537c7-65b9-4ba0-a9bb-70e81810f983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c57aa0f5-8945-4c4f-baac-d33aa11450a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural', 'language', 'processing', 'is', 'fun', '!']\n"
     ]
    }
   ],
   "source": [
    "text = \"Natural language processing is fun!\"\n",
    "doc = nlp(text)\n",
    "\n",
    "tokens = [token.text for token in doc]\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7575664c",
   "metadata": {},
   "source": [
    "- https://chatgpt.com/share/69774296-028c-8000-b5b4-8c6117c1ee17\n",
    "- https://gemini.google.com/share/322d8b970a8e"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21470992-244d-4579-8225-4cb6cf4afb9c",
   "metadata": {},
   "source": [
    "# Text Normalization\n",
    "\n",
    "Text normalization involves transforming text into a more uniform format to improve the performance of text analysis algorithms. Two common text normalization techniques are *stemming* and *lemmatization*.\n",
    "\n",
    "## Stemming\n",
    "\n",
    "Stemming is a process of reducing words to their word stem, base, or root form—generally a written word form. The idea is to remove affixes (prefixes and suffixes) from words to get to the core meaning of the word.\n",
    "\n",
    "Stemming algorithms work by cutting off the end or the beginning of the word, taking into account a list of common prefixes and suffixes that can be found in an inflected word. This process is quite crude and a stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, even if this stem is not in itself a valid root.\n",
    "\n",
    "### Porter Stemmer\n",
    "\n",
    "The Porter Stemming Algorithm is one of the oldest and most commonly used algorithms. It's designed for the English language and has a series of rules to determine the stripping of suffixes.\n",
    "\n",
    "### Snowball Stemmer\n",
    "\n",
    "The Snowball Stemmer, also known as the English Stemmer or Porter2 Stemmer, is a slightly improved version of the Porter stemmer and is part of a larger framework called Snowball. It offers stemmers for several languages besides English.\n",
    "\n",
    "### Advantages and Disadvantages of Stemming\n",
    "\n",
    "**Advantages:**\n",
    "- Simple to implement and fast to run.\n",
    "- Reduces the corpus of words the model is exposed to.\n",
    "- Often improves the performance of text classification models.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Can produce stems that are not actual words.\n",
    "- Sometimes too aggressive, cutting off too much of the word and changing the meaning.\n",
    "- Does not consider the context of the word, which can lead to inaccuracies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "926b0031-0c3d-4008-aafa-8638b86028a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "399ccb3c-13f2-4f56-8d90-cacdfd7359aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize stemmers\n",
    "porter = PorterStemmer()\n",
    "snowball = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1041d396",
   "metadata": {},
   "source": [
    "- https://www.nltk.org/api/nltk.stem.porter.html\n",
    "\n",
    "- https://www.nltk.org/api/nltk.stem.SnowballStemmer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e20d0860-992a-456b-87a4-a2af568c9add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter Stemmer: ['run', 'runner', 'run', 'ran', 'run', 'easili', 'fairli', 'better', 'mice', 'feet']\n",
      "Snowball Stemmer: ['run', 'runner', 'run', 'ran', 'run', 'easili', 'fair', 'better', 'mice', 'feet']\n"
     ]
    }
   ],
   "source": [
    "words = ['run', 'runner', 'running', 'ran', 'runs', 'easily', 'fairly', 'better', 'mice', 'feet']\n",
    "\n",
    "porter_stems = [porter.stem(word) for word in words]\n",
    "print(f\"Porter Stemmer: {porter_stems}\")\n",
    "\n",
    "snowball_stems = [snowball.stem(word) for word in words]\n",
    "print(f\"Snowball Stemmer: {snowball_stems}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6459865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'run', 'studi', 'studi']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from chatgpt\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"studies\", \"studying\"]\n",
    "\n",
    "[stemmer.stem(w) for w in words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "213f0dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'run', 'ran', 'studi', 'studi']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# my test\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"runs\", \"ran\", \"studies\", \"studying\"]\n",
    "\n",
    "[stemmer.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff3a6cd-45fe-4e87-9b58-9f2e3f28982b",
   "metadata": {},
   "source": [
    "## Lemmatization\n",
    "\n",
    "Lemmatization is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word's lemma, or dictionary form.\n",
    "\n",
    "While stemming often involves rule-based chopping of ends of words, lemmatization involves a linguistic approach to reduce a word to its base or root form. Lemmatization uses vocabulary and morphological analysis, often with the aid of part-of-speech tagging, to return the base or dictionary form of a word, known as the lemma.\n",
    "\n",
    "### The Role of Part-of-Speech Tagging in Lemmatization\n",
    "\n",
    "Part-of-speech (POS) tagging is crucial in lemmatization because many words have different lemmas based on their part of speech in a sentence. For example, the word \"saw\" can be a verb or a noun, and the lemma would differ accordingly (\"see\" for the verb, \"saw\" for the noun).\n",
    "\n",
    "`nltk` or `spacy` contain pre-trained models for POS tagging and lemmatization.\n",
    "\n",
    "### Advantages and Disadvantages of Lemmatization\n",
    "\n",
    "**Advantages:**\n",
    "- Produces lemmas, which are actual words, improving interpretability.\n",
    "- More accurate than stemming as it considers the context.\n",
    "\n",
    "**Disadvantages:**\n",
    "- More computationally expensive than stemming.\n",
    "- Requires additional information (POS tags).\n",
    "- May not improve performance significantly more than stemming for some applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ee9e22-0907-4a5f-ba77-1ad2db3a87f9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/ds0/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to /Users/ds0/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde79661",
   "metadata": {},
   "source": [
    "- https://www.nltk.org/api/nltk.stem.WordNetLemmatizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "351ee21f-7a5e-43fa-9f6c-7ce3d16c9883",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b7136553-ced5-4280-bf20-7a5319c4b1f6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to convert POS tag to a format recognized by the lemmatizer\n",
    "def get_wordnet_pos(word):\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "\n",
    "    return tag_dict.get(tag, wordnet.NOUN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09b374d0-0b19-4ae0-b426-beec511f22dd",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words: ['run', 'run', 'ran', 'study', 'study']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize words with POS tags\n",
    "lemmatized_words = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in words]\n",
    "print(\"Lemmatized Words:\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "29b8b01d-d58d-4491-abf2-18bce5821101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use spacy on another example to demonstrate pos and lemmatization\n",
    "sentence = \"The striped bats are hanging on their feet for better sleep.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2df23730-5b01-48e7-b343-18717682b131",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "30d8159f-65cf-46e0-8abf-61da4d733d84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text     Lemma    POS    Tag    Explanation\n",
      "\n",
      "The      the      DET    DT     determiner\n",
      "striped  striped  ADJ    JJ     adjective (English), other noun-modifier (Chinese)\n",
      "bats     bat      NOUN   NNS    noun, plural\n",
      "are      be       AUX    VBP    verb, non-3rd person singular present\n",
      "hanging  hang     VERB   VBG    verb, gerund or present participle\n",
      "on       on       ADP    IN     conjunction, subordinating or preposition\n",
      "their    their    PRON   PRP$   pronoun, possessive\n",
      "feet     foot     NOUN   NNS    noun, plural\n",
      "for      for      ADP    IN     conjunction, subordinating or preposition\n",
      "better   well     ADJ    JJR    adjective, comparative\n",
      "sleep    sleep    NOUN   NN     noun, singular or mass\n",
      ".        .        PUNCT  .      punctuation mark, sentence closer\n"
     ]
    }
   ],
   "source": [
    "# POS tagging and lemmatization\n",
    "print(f\"{'Text':{8}} {'Lemma':{8}} {'POS':{6}} {'Tag':{6}} {'Explanation'}\")\n",
    "print()\n",
    "for token in doc:\n",
    "    print(f\"{token.text:{8}} {token.lemma_:{8}} {token.pos_:{6}} {token.tag_:{6}} {spacy.explain(token.tag_)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce78d490-86b4-4faf-a120-fc757e1840bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the striped bat be hang on their foot for well sleep .\n"
     ]
    }
   ],
   "source": [
    "# Output the lemmatized form of each word\n",
    "lemmatized_sentence = \" \".join([token.lemma_ for token in doc])\n",
    "print(lemmatized_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dd7619b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'the'),\n",
       " ('children', 'child'),\n",
       " ('were', 'be'),\n",
       " ('running', 'run'),\n",
       " ('faster', 'fast')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from chatgpt\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "doc = nlp(\"The children were running faster\")\n",
    "[(token.text, token.lemma_) for token in doc]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8d2045",
   "metadata": {},
   "source": [
    "https://chatgpt.com/share/69774296-028c-8000-b5b4-8c6117c1ee17"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c655ff",
   "metadata": {},
   "source": [
    "## End of Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "036d3ca9-a395-46ab-a0dc-6ccf8115629c",
   "metadata": {},
   "source": [
    "# Language Modeling\n",
    "\n",
    "Language modeling is a critical task that deals with predicting the probability of a sequence of words. It is used in various applications such as speech recognition, machine translation, and text generation.\n",
    "\n",
    "## Formula\n",
    "\n",
    "A language model is a probabilistic model that assigns a probability to a sequence of words, effectively capturing the likelihood that the sequence will occur in a language. In mathematical terms, given a sequence of words $ w_1, w_2, \\ldots, w_n $, the language model estimates the probability:\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) $$\n",
    "\n",
    "This probability can be decomposed using the chain rule of probability as:\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) = P(w_1) \\cdot P(w_2 | w_1) \\cdot \\ldots \\cdot P(w_n | w_1, w_2, \\ldots, w_{n-1}) $$\n",
    "\n",
    "## N-gram Models\n",
    "\n",
    "An n-gram is a contiguous sequence of $ n $ items from a given sample of text or speech. The items can be phonemes, syllables, letters, words, or base pairs according to the application. In the context of language modeling, we are typically talking about words. It approximates the probability of a word sequence by only considering the $ n-1 $ previous words. This is known as the Markov assumption.\n",
    "\n",
    "### Unigram Models\n",
    "\n",
    "A unigram model is the simplest form of a statistical language model. It assumes that the probability of a word is independent of the words before it.\n",
    "\n",
    "$$ P(w_1, w_2, \\ldots, w_n) = P(w_1) \\cdot P(w_2) \\cdot \\ldots \\cdot P(w_n) $$\n",
    "\n",
    "### Bigram Models\n",
    "\n",
    "A bigram model, also known as a 2-gram model, assumes that the probability of a word depends only on the immediately preceding word.\n",
    "\n",
    "$$ P(w_n | w_1, w_2, \\ldots, w_{n-1}) \\approx P(w_n | w_{n-1}) = \\frac{Count(w_{n-1}, w_n)}{Count(w_{n-1})} $$\n",
    "\n",
    "### Trigram Models and Higher-Order Models\n",
    "\n",
    "Trigram models extend this to consider the two preceding words, and higher-order models consider more history. However, as the history increases, these models become more complex and require more data to estimate the probabilities accurately.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "**Sparsity**: As $ n $ increases, the likelihood of encountering unseen n-grams (those not present in the training corpus) increases, leading to sparsity.\n",
    "\n",
    "**Curse of Dimensionality**: The number of possible n-grams increases exponentially with $ n $, which leads to a combinatorial explosion in the number of parameters to be estimated.\n",
    "\n",
    "### Smoothing Techniques\n",
    "\n",
    "Smoothing techniques are used to handle the issue of zero probabilities for unseen n-grams. Common techniques include:\n",
    "\n",
    "- **Add-One (Laplace) Smoothing**: Adding one to all the n-gram counts.\n",
    "- **Add-k Smoothing**: Adding a small constant $ k $ to the counts.\n",
    "- **Backoff and Interpolation**: Using lower-order n-gram probabilities when higher-order n-grams have zero counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ad8b59a7-418d-480b-9e3b-3c14f6e0eb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86ea856",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/nlp/generate-bigrams-with-nltk/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71e84026-51e3-438b-9426-7e3cc8ca44ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample corpus\n",
    "corpus = \"I am Sam. Sam I am. I do not like green eggs and ham.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4aa8cae6-59b4-40a6-b9a2-7d0113d4f960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the corpus\n",
    "tokens = nltk.word_tokenize(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a24f20d-06f0-43e3-b307-7db3e98c8061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bigram frequencies\n",
    "bigram_freqs = Counter(bigrams(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e02f307c-accf-47f5-80e5-b77728b1cf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total number of bigrams\n",
    "total_bigrams = sum(bigram_freqs.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "893b4af3-5f8e-4766-b4d6-e156c5635b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate bigram probabilities\n",
    "bigram_probs = {bigram: freq / total_bigrams for bigram, freq in bigram_freqs.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7d39622f-e3ad-45ac-94cd-fd82d70e3c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of ('I', 'am'): 0.125\n",
      "Probability of ('am', 'Sam'): 0.0625\n",
      "Probability of ('Sam', '.'): 0.0625\n",
      "Probability of ('.', 'Sam'): 0.0625\n",
      "Probability of ('Sam', 'I'): 0.0625\n",
      "Probability of ('am', '.'): 0.0625\n",
      "Probability of ('.', 'I'): 0.0625\n",
      "Probability of ('I', 'do'): 0.0625\n",
      "Probability of ('do', 'not'): 0.0625\n",
      "Probability of ('not', 'like'): 0.0625\n",
      "Probability of ('like', 'green'): 0.0625\n",
      "Probability of ('green', 'eggs'): 0.0625\n",
      "Probability of ('eggs', 'and'): 0.0625\n",
      "Probability of ('and', 'ham'): 0.0625\n",
      "Probability of ('ham', '.'): 0.0625\n"
     ]
    }
   ],
   "source": [
    "# Display bigram probabilities\n",
    "for bigram, prob in bigram_probs.items():\n",
    "    print(f\"Probability of {bigram}: {prob}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00be94fb-c2f3-4219-a92a-85aa65cc5c0c",
   "metadata": {},
   "source": [
    "# Vector Space Model\n",
    "\n",
    "The Vector Space Model (VSM) is a mathematical model used to represent text documents as vectors of identifiers, such as index terms. It is used in information retrieval and text mining to measure the similarity between documents. In VSM, each dimension corresponds to a separate term, and the value in each dimension represents the significance of the term in the document.\n",
    "\n",
    "## Term-Document Matrix\n",
    "\n",
    "In VSM, a Term-Document Matrix is a mathematical representation of a text corpus. It describes the frequency of terms that occur in the collection of documents. In a Term-Document Matrix, rows correspond to terms in the corpus while columns correspond to documents. Each entry in this matrix denotes the frequency or the weight of a term in a document.\n",
    "\n",
    "Here's a simple example of a Term-Document Matrix for five documents:\n",
    "\n",
    "![tf](../assets/tf.png)\n",
    "\n",
    "Such matrices are often sparse since not all words appear in all documents.\n",
    "\n",
    "## TF-IDF\n",
    "\n",
    "Term Frequency-Inverse Document Frequency (TF-IDF) is a weight often used in information retrieval and text mining. This weight is a statistical measure used to evaluate the importance of a word to a document in a collection or corpus. The importance increases proportionally to the number of times a word appears in the document but is offset by the frequency of the word in the corpus.\n",
    "\n",
    "The TF-IDF value is calculated as follows:\n",
    "\n",
    "- **Term Frequency (TF)**, which measures how frequently a term occurs in a document. It is calculated as the number of times a term `t` appears in a document `d`, divided by the total number of terms in the document.\n",
    "\n",
    "$$ TF(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d} $$\n",
    "\n",
    "- **Inverse Document Frequency (IDF)**, which measures how important a term is within the entire corpus. It is calculated as the logarithm of the number of documents in the corpus divided by the number of documents where the term `t` appears.\n",
    "\n",
    "$$ IDF(t, D) = \\log \\left( \\frac{\\text{Total number of documents in corpus } D}{\\text{Number of documents with term } t} \\right) $$\n",
    "\n",
    "- **TF-IDF**, the product of TF and IDF:\n",
    "\n",
    "$$ TF\\text{-}IDF(t, d, D) = TF(t, d) \\times IDF(t, D) $$\n",
    "\n",
    "## Cosine Similarity\n",
    "\n",
    "Cosine similarity is a measure used to determine how similar two documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space. This metric is, therefore, a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude.\n",
    "\n",
    "The cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1]. The cosine similarity of two documents will range from 0 to 1, where 0 means no similarity and 1 means the same content.\n",
    "\n",
    "The formula for calculating the cosine similarity between two vectors $ A $ and $ B $ is:\n",
    "\n",
    "$$ \\text{Cosine Similarity}(A, B) = \\frac{A \\cdot B}{\\|A\\| \\|B\\|} = \\frac{\\sum_{i=1}^{n} A_i \\times B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}} $$\n",
    "\n",
    "## Limitations\n",
    "\n",
    "- Assumes term independence, which is not always the case.\n",
    "- Does not capture the semantic relationship between words.\n",
    "- High-dimensional and sparse vectors due to the size of the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "52c8953a-2faa-4755-b6dc-37f80fddd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc8b1e",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0d6856ef-bedf-4c51-8f9b-db4f6a0d9252",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample documents\n",
    "documents = [\n",
    "    'The sky is blue',\n",
    "    'The sun is bright',\n",
    "    'The sun in the sky is bright',\n",
    "    'We can see the shining sun, the bright sun'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07d0c98d-f291-4d27-a151-eebfcf8b05ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a1b5897a-7f40-4fb4-aae1-e01ce9397d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b260969c-be3e-46b1-bb43-c897fa88bd77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.65919112 0.         0.         0.         0.42075315 0.\n",
      "  0.         0.51971385 0.         0.34399327 0.        ]\n",
      " [0.         0.52210862 0.         0.         0.52210862 0.\n",
      "  0.         0.         0.52210862 0.42685801 0.        ]\n",
      " [0.         0.3218464  0.         0.50423458 0.3218464  0.\n",
      "  0.         0.39754433 0.3218464  0.52626104 0.        ]\n",
      " [0.         0.23910199 0.37459947 0.         0.         0.37459947\n",
      "  0.37459947 0.         0.47820398 0.39096309 0.37459947]]\n"
     ]
    }
   ],
   "source": [
    "# Display the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beeb949",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Names: ['blue' 'bright' 'can' 'in' 'is' 'see' 'shining' 'sky' 'sun' 'the' 'we']\n",
      "       blue    bright       can        in        is       see   shining  \\\n",
      "0  0.659191  0.000000  0.000000  0.000000  0.420753  0.000000  0.000000   \n",
      "1  0.000000  0.522109  0.000000  0.000000  0.522109  0.000000  0.000000   \n",
      "2  0.000000  0.321846  0.000000  0.504235  0.321846  0.000000  0.000000   \n",
      "3  0.000000  0.239102  0.374599  0.000000  0.000000  0.374599  0.374599   \n",
      "\n",
      "        sky       sun       the        we  \n",
      "0  0.519714  0.000000  0.343993  0.000000  \n",
      "1  0.000000  0.522109  0.426858  0.000000  \n",
      "2  0.397544  0.321846  0.526261  0.000000  \n",
      "3  0.000000  0.478204  0.390963  0.374599  \n"
     ]
    }
   ],
   "source": [
    "## Display TF-IDF matrix with feature names in pandas format\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "print(\"Feature Names:\", feature_names)\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9aa9b212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The sky is blue',\n",
       " 'The sun is bright',\n",
       " 'The sun in the sky is bright',\n",
       " 'We can see the shining sun, the bright sun']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5866e768-a9be-4448-898e-26cf8487a7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.36651513 1.         0.72875508 0.54139736]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Cosine Similarity between the 2nd document with all others\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[1], tfidf_matrix)\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bb92b21c-ecf0-469e-a761-1242ca42d38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize another TF-IDF Vectorizer, with stop words removal\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b0c27285-3f68-443c-9062-316d5498a0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorize the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "31960d6a-7e0b-4790-a657-10dd24f9c99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.78528828 0.         0.         0.6191303  0.        ]\n",
      " [0.         0.70710678 0.         0.         0.70710678]\n",
      " [0.         0.53256952 0.         0.65782931 0.53256952]\n",
      " [0.         0.36626037 0.57381765 0.         0.73252075]]\n"
     ]
    }
   ],
   "source": [
    "# Display the TF-IDF matrix\n",
    "print(tfidf_matrix.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a5e6f0c0-5c97-4a64-a69a-8a72573a0efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         1.         0.75316704 0.77695558]]\n"
     ]
    }
   ],
   "source": [
    "# Calculate Cosine Similarity between the 2nd document with all others\n",
    "cosine_similarities = cosine_similarity(tfidf_matrix[1], tfidf_matrix)\n",
    "print(cosine_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096b408b",
   "metadata": {},
   "source": [
    "- https://chatgpt.com/share/69774296-028c-8000-b5b4-8c6117c1ee17\n",
    "- https://claude.ai/share/65fc32ae-314b-4e60-80bc-5f3426b8c7f4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe902f2a",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "https://nlp.stanford.edu/IR-book/\n",
    "\n",
    "https://web.stanford.edu/~jurafsky/slp3/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea6b6c1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ba75f08b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
